# C10M

## C10K问题
Google公司Dan Kegel在《The C10K problem》中提到的：
单机超过1w并发客户端连接问题，即使硬件性能足够，依然不能正常提供服务。
现在C10K问题已有各种解决方案，很多人已经在研究C10M问题，探究C10K的解决过程，或许可以摸索到技术的发展规律，也可以借鉴来解决工程实践中的问题。

**为何早期QQ用的UDP而不是TCP？**
早期的腾讯QQ也同样面临C10K问题，只不过他们是用了UDP这种原始的包交换协议来实现的，绕开了这个难题。
如何绕开的？

Web1.0时代 静态网页，门户网站  
Web2.0时代 交互式网页，一个APP的用户TCP连接数过亿

不行就用更好的机器的思维

基于select()的OS上可以支持1000个TCP连接并发，在2倍性能的硬件服务器上却支持不了2000并发。
1. 使用select/poll，效率较低的多路复用模式（epoll还没有出来，epoll的下一代是什么？）
2. 不使用多路复用，而是为连接创建很多线程和进程，进线程上下文切换消耗大，甚至导致操作系统崩溃  
客户端请求，唤起进线程，操作系统上下文切换一次，是很糟糕的实现
3. 32位机器和操作系统的进程数/端口数限制
4. HTTP1.0的TCP短链接引发的问题

freebsd实现了kqueue，linux实现了epoll，windows实现了IOCP。
nginx/libevent/node.js及后来的openresty/golang都是基于他们的产物。
通过select/poll->epoll来看，OS内核似乎是解决问题的关键，反思之，内核也是问题所在

进线程切换很消耗CPU，是否有更轻量级的呢，不会OS调度，无锁调度？  
协程coroutine  
Lua/Python的coroutine，Go的goroutine都是类似概念。实际C/C++可以实现类似模型。使用一组线程（内核数相关）来处理多个请求，当某次处理阻塞，则继续处理其他请求，避免大量上下文的交换。每个协程所独占的系统资源只有栈部分。协程的切换，是代码控制的（类似callback的），不需要内核参与，实现异步。
本质上是异步非阻塞技术，将阻塞事件封装成回调，线程处理的是事件循环。比如，recv()函数实际是把当前执行位置、局部变量保存起来，然后去处理别的请求，等网络数据到来，再把刚才的执行位置和局部变量取出继续执行。

协程是用户态进线程？  
进线程相当于操作系统做事件循环，而协程用epoll做事件循环。
协程的开销远比进线程小。

协程的缺点  
当一个请求是密集计算时，会长时间占用CPU，其他请求得不到执行。（应该可以改进）
进线程开销大，操作系统通过分配时间片使进线程并发。

C/C++的如何实现挂起当前阻塞的协程？  
所有的代码执行都可以分为不会阻塞的和会阻塞的（如访问磁盘文件、访问数据库、访问网络），将所有的会阻塞的代码（包括底层代码）都实现为回调函数（先把当前请求处理放到阻塞队列，当数据返回时把当前请求处理放到待处理的队列队尾）还不行，还需要保存当前的执行位置和局部变量。

异步非阻塞vs同步阻塞  
同步阻塞并不会浪费资源，性能也不差。阻塞后被操作系统挂起，就绪后再被执行。只是进线程创建多了以后副作用很大，进线程切换耗费了相当的资源。当只有1K的并发连接，同步阻塞比异步非阻塞体验还好一些。

异步回调VS异步非阻塞  
完全的异步回调是没有切换开销的，等同于顺序执行代码，实现比较难。异步非阻塞还是需要调度，需要消耗的。

[epoll和select/poll区别](epoll.md)

**单节点问题**
如单数据库/单登录节点等等

## 高性能I/O之Reactor模式 netty redis
高效处理大量连接的事件驱动模型

1. 原型
```cpp
while (true) {
    socket = accept();
    handle(socket);
}
```
无并发，阻塞，吞吐量低

2. connection per thread
```cpp
while (true) {
    socket = accept();
    new thread(handler(socket));
}
```
tomcat早期模式，资源耗费高，支持的并发连接数量少

3. 线程池化  
池内每个线程做三件事：连接、读取、写入。

线程同步粒度限制了吞吐量，应把每次连接的操作分为更细的粒度或过程。池内线程更多，处理更简单，职责更单一。

4. reactor  
拆分成多个线程任务或子过程（也即handler，每个handler处理一种event）。
有一个全局管理者selector，在channel上注册关注的事件，selector就不断去检测channel上的事件是否发生，没有则阻塞，有则调用相应的handler处理。如连接读取和写入。

+ 原型  
一个线程处理accept；其他线程处理请求。  
每个handle都会这样处理：读取请求内容=>解码=>计算=>编码=>回复。I/O阻塞时，当前线程一直等待，其他用户的请求却得不到响应（表现为负载增加，性能下降）。
阻塞点在：1. accept()函数，等待新连接；2. read函数等待数据接收读取完；3.write函数等待数据发送完。

+ 改进：基于事件驱动的设计   
当有事件触发时，才会调用处理器进行处理（基于操作系统事件就绪通知信号？）  
一个reactor线程负责响应I/O事件，把就绪的事件发送给handler处理；handler负责执行非阻塞的处理。

+ 改进：处理器的执行（非事件的执行）交给线程池处理  
多线程处理解码、计算、编码等
+ 改进：多reactor（依据事件类型分组）  
mainReactor负责监听连接，处理accept函数的新连接；subReactor处理recv/send事件；

### 模型结构
reactor设计模式是为了处理大量输入并发的服务请求的一种事件驱动模型。

事件驱动/大量并发输入源/service-handler/多个request-handler

service-handler会同步地将输入的请求（event）通过多路复用分发给相应的request-handler

#### 组成
1. Initiation Dispatcher  
   handler_events()/register_handler(h)/remove_handler(h)
   handler_events()即多路复用分发请求
2. Synchronous Event Demultiplexer多路信号分离器  
    select/poll/epoll
3. Handler  
   Event Handler：handle_event(type)/get_handle()  
   Concrete/Event/Handler

event：打开的文件、socket、timer等的read/write/close事件

大文件传输时，使用thread-per-connection更好，或者使用proactor模式。

## Proactor
reactor的问题：
+ reactor性能确实非常高，适合高并发，但是使用的是同步IO
+ 同步IO需要等待内核准备好数据，此时，线程是阻塞的
    + 如果线程阻塞时间长，且高并发地阻塞，多线程切换，必然整体性能不高

Proactor 关注的是完成事件（而不是reactor的就绪事件）。

+ Procator Initiator：负责创建Handler和Procator，并将Procator和Handler都通过Asynchronous operation processor注册到内核。
+ Handler：执行业务流程的业务处理器。
+ Asynchronous operation processor：负责处理注册请求，并完成IO操作。完成IO操作后会通知Procator。
+ Procator：根据不同的事件类型回调不同的handler进行业务处理。

Linux 异步I/O 不完善，windows的IOCP实现是真正意义的异步I/O，windows里实现的Proactor更高效。

+ 优点，性能确实强大，效率也高
+ 缺点
    + 复杂。
    + 操作系统支持。linux支持不好

Proactor模式与Reactor模式 的区别有如下几点：
+ Reactor 模式注册的是文件描述符的就绪事件。当Reactor 模式有事件发生时，它需要判断当前事件是读事件还是写事件，然后在调用系统的read或者write将数据从内核中拷贝到用户数据区，然后进行业务处理。
+ Proactor模式注册的则是完成事件。即发起异步操作后，操作系统将在内核态完成I/O并拷贝数据到用户提供的缓冲区中，完成后通知Proactor进行回调，用户只需要处理后续的业务即可。
+ Reactor模式实现同步I/O多路分发
+ Proactor模式实现异步I/O分发。

在 Linux 操作系统下实现高并发网络编程依然以Reactor 模式为主。
### 模型结构


# C10M
## 模拟百万连接
